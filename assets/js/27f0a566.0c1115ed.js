"use strict";(self.webpackChunkgg_interview_document=self.webpackChunkgg_interview_document||[]).push([[2410],{9569:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>p,default:()=>c,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"project-docs/enhanced-sequence-diagram","title":"Enhanced System Sequence Diagrams","description":"Complete User Flow with LLMs and Media Integration","source":"@site/docs/project-docs/enhanced-sequence-diagram.md","sourceDirName":"project-docs","slug":"/project-docs/enhanced-sequence-diagram","permalink":"/gg-interview-document-website/project-docs/enhanced-sequence-diagram","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Project Charter","permalink":"/gg-interview-document-website/project-docs/project-charter"}}');var o=i(4848),r=i(8453);const a={sidebar_position:4},p="Enhanced System Sequence Diagrams",s={},l=[{value:"Complete User Flow with LLMs and Media Integration",id:"complete-user-flow-with-llms-and-media-integration",level:2},{value:"Problem and Solution Flow Details",id:"problem-and-solution-flow-details",level:2},{value:"LeetCode GraphQL Integration Flow",id:"leetcode-graphql-integration-flow",level:2},{value:"Real-time WebSocket and Media Communication",id:"real-time-websocket-and-media-communication",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",mermaid:"mermaid",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"enhanced-system-sequence-diagrams",children:"Enhanced System Sequence Diagrams"})}),"\n",(0,o.jsx)(n.h2,{id:"complete-user-flow-with-llms-and-media-integration",children:"Complete User Flow with LLMs and Media Integration"}),"\n",(0,o.jsx)(n.mermaid,{value:'sequenceDiagram\n    actor User\n    participant WebApp as Web Application (Port 3500)\n    participant GeminiWS as Gemini WebSocket\n    participant GeminiAI as Gemini AI Models\n    participant ProblemService as Problem Service (Port 3000)\n    participant AIAgent as AI Agent Service (Port 8000)\n    participant OLLM as OpenAI/Anthropic LLM\n    \n    %% Initial setup and problem selection\n    User->>WebApp: Access interview platform\n    WebApp->>User: Display UI with problem selection\n    User->>WebApp: Select problem\n    WebApp->>ProblemService: Request problem details\n    ProblemService--\x3e>WebApp: Return problem data\n    \n    %% Solution fetching flow\n    WebApp->>AIAgent: Request solution generation\n    AIAgent->>OLLM: Pass problem to LLM with structured prompt\n    Note over OLLM: Generate multiple solution approaches\n    OLLM--\x3e>AIAgent: Return structured solutions\n    AIAgent--\x3e>WebApp: Return formatted solutions\n    Note over WebApp: Store solutions locally\n    \n    %% Real-time interview setup\n    User->>WebApp: Start interview session\n    WebApp->>User: Request screen/camera access\n    User--\x3e>WebApp: Grant media permissions\n    \n    %% WebSocket connection\n    WebApp->>GeminiWS: Establish WebSocket connection\n    GeminiWS->>GeminiAI: Send initial setup prompt\n    Note over GeminiAI: Configure as coding interviewer\n    \n    %% Problem context sharing\n    WebApp->>GeminiWS: Send problem context\n    GeminiWS->>GeminiAI: Pass problem details\n    GeminiAI--\x3e>GeminiWS: Send greeting and initial guidance\n    GeminiWS--\x3e>WebApp: Return text response\n    WebApp--\x3e>User: Display interviewer message\n    \n    %% Screen sharing loop\n    loop Every second\n        WebApp->>WebApp: Capture screen/camera frame\n        WebApp->>GeminiWS: Send base64 encoded image\n        GeminiWS->>GeminiAI: Pass image data\n    end\n    \n    %% Audio communication loop\n    loop Real-time audio\n        User->>WebApp: Speak (audio input)\n        WebApp->>WebApp: Process audio with AudioWorklet\n        WebApp->>GeminiWS: Send PCM audio chunks\n        GeminiWS->>GeminiAI: Pass audio data\n        \n        opt Transcription (periodic)\n            WebApp->>GeminiAI: Send audio for transcription\n            GeminiAI--\x3e>WebApp: Return transcribed text\n            WebApp--\x3e>User: Display transcription\n        end\n        \n        GeminiAI--\x3e>GeminiWS: Generate response (text)\n        GeminiWS--\x3e>WebApp: Return text response\n        WebApp--\x3e>User: Display response\n        \n        GeminiAI--\x3e>GeminiWS: Generate audio response\n        GeminiWS--\x3e>WebApp: Return audio data\n        WebApp--\x3e>User: Play audio response\n    end\n    \n    %% Solution discussion\n    User->>WebApp: Request solution hint\n    Note over WebApp: Access stored solutions\n    WebApp->>GeminiWS: Send "solution available" context\n    GeminiWS->>GeminiAI: Update with solution awareness\n    GeminiAI--\x3e>GeminiWS: Generate hint based on solutions\n    GeminiWS--\x3e>WebApp: Return hint text\n    WebApp--\x3e>User: Display hint\n    \n    %% Session end\n    User->>WebApp: End interview session\n    WebApp->>GeminiWS: Close WebSocket connection\n    WebApp->>WebApp: Release media resources'}),"\n",(0,o.jsx)(n.h2,{id:"problem-and-solution-flow-details",children:"Problem and Solution Flow Details"}),"\n",(0,o.jsx)(n.mermaid,{value:'sequenceDiagram\n    actor User\n    participant WebApp as Web Application (Port 3500)\n    participant ProblemService as Problem Service (Port 3000)\n    participant AIAgent as AI Agent Service (Port 8000)\n    participant OLLM as OpenAI/Anthropic LLM\n    \n    %% Problem selection\n    User->>WebApp: Select problem\n    WebApp->>ProblemService: GET /api/problems/:id\n    ProblemService--\x3e>WebApp: Return problem object\n    WebApp--\x3e>User: Display problem\n    \n    %% Solution generation (background)\n    WebApp->>AIAgent: POST /api/leetcode-solutions\n    Note over AIAgent: Create LeetCode Agent\n    AIAgent->>OLLM: Send problem with structured prompt\n    \n    Note over OLLM: Process with language-specific guidance:<br/>1. Analyze problem<br/>2. Develop multiple approaches<br/>3. Generate structured output\n    \n    OLLM--\x3e>AIAgent: Return function-calling JSON response\n    Note over AIAgent: Parse and format solutions\n    AIAgent--\x3e>WebApp: Return solution object\n    \n    Note over WebApp: Store solutions locally<br/>Solutions structure:<br/>- Introduction<br/>- Multiple approaches<br/>- Time/space complexity<br/>- Code implementations<br/>- Edge cases\n    \n    %% Solution access (when requested)\n    User->>WebApp: Ask for hint\n    WebApp->>WebApp: Access stored solutions\n    WebApp->>GeminiWS: "Solutions loaded" context message\n    WebApp--\x3e>User: Display hint based on stored solution'}),"\n",(0,o.jsx)(n.h2,{id:"leetcode-graphql-integration-flow",children:"LeetCode GraphQL Integration Flow"}),"\n",(0,o.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant ProblemService as Problem Service (Port 3000)\n    participant LeetCodeAPI as LeetCode GraphQL API\n    participant Cache as Problem Cache\n    \n    %% Initial problem sync process\n    Note over ProblemService: On service startup\n    ProblemService->>LeetCodeAPI: POST GraphQL query to fetch problems\n    Note over LeetCodeAPI: Endpoint: https://leetcode.com/graphql\n    \n    %% GraphQL query details\n    Note over ProblemService,LeetCodeAPI: Query:<br/>{ problemsetQuestionList {<br/>  questions {<br/>    questionId<br/>    title<br/>    difficulty<br/>    topicTags { name }<br/>    content<br/>  }<br/>}}\n    \n    LeetCodeAPI--\x3e>ProblemService: Return problems list JSON\n    ProblemService->>Cache: Store problem metadata and content\n    \n    %% Detailed problem fetch\n    loop For each problem\n        ProblemService->>LeetCodeAPI: POST GraphQL query for problem details\n        Note over ProblemService,LeetCodeAPI: Query:<br/>{ question(titleSlug: "$slug") {<br/>  questionId<br/>  title<br/>  content<br/>  difficulty<br/>  exampleTestcases<br/>  codeSnippets { langSlug, code }<br/>}}\n        \n        LeetCodeAPI--\x3e>ProblemService: Return detailed problem data\n        ProblemService->>Cache: Update problem with full details\n    end\n    \n    %% Problem API endpoint\n    Note over ProblemService: Expose REST API:<br/>/api/problems<br/>/api/problems/:id\n    \n    %% Periodic refresh process\n    Note over ProblemService: Daily scheduled job\n    ProblemService->>LeetCodeAPI: POST GraphQL query for updates\n    LeetCodeAPI--\x3e>ProblemService: Return updated problems\n    ProblemService->>Cache: Update changed problems'}),"\n",(0,o.jsx)(n.h2,{id:"real-time-websocket-and-media-communication",children:"Real-time WebSocket and Media Communication"}),"\n",(0,o.jsx)(n.mermaid,{value:"sequenceDiagram\n    actor User\n    participant WebApp as Web Application\n    participant AudioAPI as Web Audio API\n    participant MediaAPI as MediaDevices API\n    participant GeminiWS as Gemini WebSocket\n    participant GeminiAI as Gemini AI\n    \n    User->>WebApp: Start interview session\n    WebApp->>MediaAPI: getUserMedia() or getDisplayMedia()\n    MediaAPI--\x3e>WebApp: Return media stream\n    \n    WebApp->>GeminiWS: new WebSocket(GEMINI_WS_URL)\n    GeminiWS->>GeminiAI: Initial setup message\n    Note over GeminiAI: Configure as coding interviewer\n    \n    %% Setup audio processing\n    WebApp->>AudioAPI: new AudioContext()\n    WebApp->>AudioAPI: createAudioWorkletNode()\n    Note over AudioAPI: Audio processor worklet<br/>- Capture audio<br/>- Calculate audio levels<br/>- Buffer PCM data\n    \n    par Audio Channel\n        loop Audio Processing\n            User->>WebApp: Speak (audio input)\n            WebApp->>AudioAPI: Process audio input\n            AudioAPI->>WebApp: Return PCM data chunks\n            WebApp->>WebApp: Convert to Base64\n            WebApp->>GeminiWS: Send audio data\n            GeminiWS->>GeminiAI: Process audio input\n            \n            GeminiAI--\x3e>GeminiWS: Generate audio response\n            GeminiWS--\x3e>WebApp: Return Base64 audio\n            WebApp->>AudioAPI: Decode and queue audio\n            AudioAPI--\x3e>User: Play audio response\n        end\n    and Video Channel\n        loop Every 1000ms\n            WebApp->>MediaAPI: Capture video frame\n            MediaAPI->>WebApp: Return frame\n            WebApp->>WebApp: Convert to Base64\n            WebApp->>GeminiWS: Send image data\n            GeminiWS->>GeminiAI: Process visual input\n        end\n    and Transcription Channel\n        loop Rate-limited (every 2s)\n            WebApp->>GeminiAI: Send audio for transcription\n            Note over GeminiAI: Using gemini-1.5-pro model\n            GeminiAI--\x3e>WebApp: Return transcribed text\n            WebApp--\x3e>User: Display transcription\n        end\n    end\n    \n    User->>WebApp: End session\n    WebApp->>GeminiWS: Close connection\n    WebApp->>AudioAPI: Close AudioContext\n    WebApp->>MediaAPI: Stop all tracks"})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);